# AI Use Disclosure

This study is submitted explicitly as a test case for responsible AI use in academic research. The author believes that language models, when wielded with appropriate oversight and verification, offer tremendous potential for democratizing sophisticated research assistance. However, this potential can only be realized through rigorous attention to transparency, verification, and ethical implementation. The present work represents the author's best attempt at demonstrating such responsible use while acknowledging that best practices in this rapidly evolving field remain under active development.

The author also recognizes that the Journal of Spatial Information Science currently asks authors to certify that their submissions were composed entirely by the human authors and that any use of language technology be confined to superficial grammatical polishing. The present manuscript departs from that requirement because large language models contributed in substantive, though supervised, ways to code development, methodological design, and segment-level drafting. The work is submitted in full view of this divergence, accompanied by the present, detailed disclosure, so that editors and reviewers can evaluate in an evidence-based manner whether the transparency, verification procedures, and scholarly merit demonstrated here justify an exception or, alternatively, indicate that existing policies should be revisited to reflect emerging research practices.

The author submits this work with intellectual humility, recognizing that responsible AI use in research is an evolving challenge requiring ongoing community engagement rather than an individual solution.

## How this research uses artificial intelligence

This research employed large language models, primarily Claude 3.7 Sonnet (Anthropic) and o3 (OpenAI), extensively as research assistants throughout the study, while maintaining strict human oversight, verification, and control over all scientific content and conclusions.

Claude 3.7 Sonnet assisted with software development tasks including debugging Python scripts, writing analysis code based on author-specified requirements, troubleshooting API integration issues, and resolving compatibility problems between different software libraries.

Claude 3.7 Sonnet wrote substantial portions of the accuracy analysis scripts (such as extended_accuracy_stats.py) based on detailed specifications and requirements provided by the author, with all code subsequently verified and tested to ensure correctness.

During the experimental design phase, o3 served as a methodological sounding board, helping to identify potential weaknesses in experimental approaches and suggesting improvements to ensure rigorous evaluation protocols.

Both Claude 3.7 Sonnet and o3 were used in prompt engineering.

Claude 3.7 Sonnet provided substantial assistance with LaTeX formatting, automated reference insertion, and citation management throughout the paper preparation process.

o3 helped identify areas for methodological improvement in draft sections, serving as an academic advisor to strengthen the paper's rigor, though the author explicitly avoided allowing AI to write, rewrite, or edit substantive content directly.

o3 served as a research assistant for literature review tasks, helping to analyze and understand methodologies in related papers and confirming the author's interpretations of relevant prior work.

When model outputs did not meet precise specifications or contained errors, the author iteratively refined prompts and corrected approaches until the results matched exact requirements. All AI suggestions for methodology were critically evaluated before implementation. The author treated AI as a fallible research assistant requiring constant oversight and verification.

All AI functions and limitations were determined by the author.

Every AI output was directly attributable to specific author intent and requirements.

All AI-assisted processes can be understood and clearly articulated by the author.

The author independently formulated hypotheses, designed evaluation metrics, and drew conclusions from the statistical analyses.

All writing in the manuscript represents the author's own analysis and interpretation, though the text benefited from standard spelling, grammar checking, and autocompletions typical of modern word processing software.

Any factual claims or literature interpretations suggested by AI were independently verified through primary source review. Language models were never trusted as a source of truth but rather as a starting point requiring verification.

## What AI Was Not Used For

The ground-truth dataset construction relied entirely on traditional archival methods, professional GIS analysis, and expert historical research.

The professional GIS benchmark was completed by an independent contractor using conventional geospatial methods with explicit instructions to avoid any AI assistance during script development, execution, or result evaluation.


## Risks and Areas of Concern

The author acknowledges several significant limitations and potential concerns with this AI-assisted approach.

The extensive verification required for AI-generated content may offset efficiency gains and introduce new sources of human error. The cognitive load of constantly evaluating AI outputs may paradoxically reduce research quality.

Even with safeguards, AI suggestions may subtly influence research directions or analytical choices in ways that compromise scientific independence. The line between AI-assisted and AI-influenced research may be less clear than this disclosure suggests.

Despite efforts to document AI interactions, the non-deterministic nature of large language models means that exact reproduction of AI-assisted processes may be impossible. This represents a fundamental challenge to traditional scientific reproducibility standards.

AI training data inevitably contains biases that may influence code architecture, visualization choices, or methodological suggestions in ways that are difficult to detect or quantify.

It is unclear whether the intensive verification approach used here would remain practical for larger-scale or more complex research projects.

Reliance on proprietary AI tools raises concerns about research accessibility and creates potential dependencies on commercial entities that may not align with open science principles.

The author acknowledges uncertainty about whether extensive AI assistance might atrophy important research skills over time, both for this author and for the broader academic community. As understanding of AI's societal impacts evolves, practices considered responsible today may prove inadequate or even harmful by future standards.

This approach, if validated, could pressure researchers to adopt AI assistance regardless of appropriateness to their specific contexts, potentially creating a technological arms race rather than thoughtful tool adoption.

## Open Questions for Academic Community

The author submits this work with full recognition that many fundamental questions about AI in research remain unresolved. What verification standards should the academic community establish for AI-assisted research? How can innovation be balanced with scientific rigor? How detailed should AI use disclosures be? At what point does transparency become burdensome without adding meaningful value?

How should peer review processes evolve to evaluate AI-assisted research? Do reviewers need specialized training to assess such work? Should AI integration standards vary across disciplines, or should universal principles govern all academic fields? How can academic standards keep pace with rapidly evolving AI capabilities without stifling beneficial innovation? Where should the academic community draw lines regarding appropriate AI assistance versus inappropriate automation of scholarly thinking?

The author explicitly invites the academic community to identify weaknesses, oversights, or inappropriate uses of AI assistance described herein. The author welcomes criticism that improves future practice. The academic community should evaluate whether the responsible AI principles applied here are sufficient, appropriate, or even necessary for academic research contexts.

The author commits to engaging openly with any questions, criticisms, or suggestions that emerge from peer review or broader academic discussion of these methods.
